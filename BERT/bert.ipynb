{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is BERT\n",
    "\n",
    "Bidirectional Encoder Representation from Transformers\n",
    "\n",
    "- Encoder Representations: language modeling system, pre-trained with unalbeled data. Then fine-tunning\n",
    "- from Transformer: based on powerful NLP algorithm. Defines the architecture of BERT.\n",
    "- Bidirectional: uses with left and right context when dealing with a word. Defines the training process.\n",
    "\n",
    "- ELMo = bidirectional with LSTM\n",
    "- OpenAi GPT = Transformers from left to right only\n",
    "- BERT both (transformers + bidirectional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link https://medium.com/@wwydmanski/whats-the-difference-between-self-attention-and-attention-in-transformer-architecture-3780404382f3\n",
    "\n",
    "Self-attention and attention are both mechanisms that allow transformer models to attend to different parts of the input or output sequences when making predictions. These mechanisms are crucial for the performance of transformer models in tasks such as language translation, text summarization, and sentiment analysis, where the model needs to understand the relationships between different words or phrases in the input and output sequences.\n",
    "\n",
    "Attention refers to the ability of a transformer model to attend to different parts of another sequence when making predictions. This is often used in encoder-decoder architectures, where the encoder vectorizes the input sequence, and the decoder attends to the encoded representation of the whole input when making predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Query (Q), Key (K), and Value (V) in Transformers\n",
    "\n",
    "## Overview\n",
    "Transformers use a self-attention mechanism to determine how much focus each token in a sequence should have on others. The three main components of self-attention are:\n",
    "\n",
    "### Query (Q)\n",
    "- The **query** represents the token that is searching for relevant information.\n",
    "- It determines **what the model is currently attending to**.\n",
    "- Each token in a sequence generates its own query vector.\n",
    "\n",
    "### Key (K)\n",
    "- The **key** represents the potential \"match\" or \"index\" for a given query.\n",
    "- It determines **how relevant other tokens are** to the current query.\n",
    "- Each token has a key vector, and the model computes the **similarity** between queries and keys.\n",
    "\n",
    "### Value (V)\n",
    "- The **value** holds the actual content or information of each token.\n",
    "- It determines **what information is retrieved** when attention is applied.\n",
    "- The value vector is used to compute the final output.\n",
    "\n",
    "## How It Works\n",
    "1. Each token in the sequence is mapped to its respective **Q, K, and V** vectors.\n",
    "2. The similarity between **Q and K** is computed using a dot product.\n",
    "3. The results are scaled and passed through a **softmax function** to generate attention scores.\n",
    "4. These scores are used to weight the **V (Value)** vectors.\n",
    "5. The final output is a weighted sum of the value vectors, emphasizing important tokens.\n",
    "\n",
    "## Mathematical Representation\n",
    "The attention mechanism is defined as:\n",
    "\n",
    "\\[\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\\]\n",
    "\n",
    "where:\n",
    "- \\(QK^T\\) computes similarity scores between queries and keys.\n",
    "- \\(\\sqrt{d_k}\\) is a scaling factor to prevent large values.\n",
    "- **Softmax** normalizes the scores.\n",
    "- The final result is multiplied with **V** to obtain the attended output.\n",
    "\n",
    "## Why It Matters\n",
    "Self-attention allows transformers to capture long-range dependencies in text, making them highly effective for NLP tasks like machine translation, text summarization, and language modeling.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
