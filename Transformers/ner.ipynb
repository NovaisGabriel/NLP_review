{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acdbfbeceb6458081c53cdf497cc909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Windows 11\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Windows 11\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Windows 11\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b65fdd48b84083aa7fa4c88c9551a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f56dfbf14f4449f8f29eed67dfca56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8fd929cdc9d4fb68782fa1900826ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "ner = pipeline(\"ner\", aggregation_strategy='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ner_train.pkl', 'rb') as f:\n",
    "  corpus_train = pickle.load(f)\n",
    "\n",
    "with open('ner_test.pkl', 'rb') as f:\n",
    "  corpus_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CRICKET', 'O'),\n",
       " ('-', 'O'),\n",
       " ('LEICESTERSHIRE', 'B-ORG'),\n",
       " ('TAKE', 'O'),\n",
       " ('OVER', 'O'),\n",
       " ('AT', 'O'),\n",
       " ('TOP', 'O'),\n",
       " ('AFTER', 'O'),\n",
       " ('INNINGS', 'O'),\n",
       " ('VICTORY', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for sentence_tag_pairs in corpus_test:\n",
    "  tokens = []\n",
    "  target = []\n",
    "  for token, tag in sentence_tag_pairs:\n",
    "    tokens.append(token)\n",
    "    target.append(tag)\n",
    "  inputs.append(tokens)\n",
    "  targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'was',\n",
       " 'well',\n",
       " 'backed',\n",
       " 'by',\n",
       " 'England',\n",
       " 'hopeful',\n",
       " 'Mark',\n",
       " 'Butcher',\n",
       " 'who',\n",
       " 'made',\n",
       " '70',\n",
       " 'as',\n",
       " 'Surrey',\n",
       " 'closed',\n",
       " 'on',\n",
       " '429',\n",
       " 'for',\n",
       " 'seven',\n",
       " ',',\n",
       " 'a',\n",
       " 'lead',\n",
       " 'of',\n",
       " '234',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "detokenizer = TreebankWordDetokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He was well backed by England hopeful Mark Butcher who made 70 as Surrey closed on 429 for seven, a lead of 234.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detokenizer.detokenize(inputs[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'LOC',\n",
       "  'score': 0.99967515,\n",
       "  'word': 'England',\n",
       "  'start': 22,\n",
       "  'end': 29},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.99974275,\n",
       "  'word': 'Mark Butcher',\n",
       "  'start': 38,\n",
       "  'end': 50},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9996264,\n",
       "  'word': 'Surrey',\n",
       "  'start': 66,\n",
       "  'end': 72}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner(detokenizer.detokenize(inputs[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prediction(tokens, input_, ner_result):\n",
    "  # map hugging face ner result to list of tags for later performance assessment\n",
    "  # tokens is the original tokenized sentence\n",
    "  # input_ is the detokenized string\n",
    "\n",
    "  predicted_tags = []\n",
    "  state = 'O' # keep track of state, so if O --> B, if B --> I, if I --> I\n",
    "  current_index = 0\n",
    "\n",
    "  # keep track of last group since the group may change\n",
    "  # between consecutive entities\n",
    "  # e.g. we want B-MISC -> B-PER -> I-PER\n",
    "  # not          B-MISC -> I-PER -> I-PER\n",
    "  last_group = None\n",
    "\n",
    "  for token in tokens:\n",
    "    # find the token in the input_ (should be at or near the start)\n",
    "    index = input_.find(token)\n",
    "    assert(index >= 0)\n",
    "    current_index += index # where we are currently pointing to\n",
    "\n",
    "    # print(token, current_index) # debug\n",
    "\n",
    "    # check if this index belongs to an entity and assign label\n",
    "    tag = 'O'\n",
    "    for entity in ner_result:\n",
    "      group = entity['entity_group']\n",
    "      if current_index >= entity['start'] and current_index < entity['end']:\n",
    "        # then this token belongs to an entity\n",
    "        if state == 'O':\n",
    "          state = 'B'\n",
    "        elif last_group != group:\n",
    "          state = 'B'\n",
    "        else:\n",
    "          state = 'I'\n",
    "        tag = f\"{state}-{group}\"\n",
    "        last_group = group\n",
    "        break\n",
    "    if tag == 'O':\n",
    "      # reset the state\n",
    "      state = 'O'\n",
    "      last_group = None\n",
    "    predicted_tags.append(tag)\n",
    "\n",
    "    # remove the token from input_\n",
    "    input_ = input_[index + len(token):]\n",
    "\n",
    "    # update current_index\n",
    "    current_index += len(token)\n",
    "\n",
    "  # sanity check\n",
    "  # print(\"len(predicted_tags)\", len(predicted_tags))\n",
    "  # print(\"len(tokens)\", len(tokens))\n",
    "  assert(len(predicted_tags) == len(tokens))\n",
    "  return predicted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = detokenizer.detokenize(inputs[9])\n",
    "ner_result = ner(input_)\n",
    "ptags = compute_prediction(inputs[9], input_, ner_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-MISC',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TMP\n",
    "input2 = detokenizer.detokenize(inputs[11])\n",
    "ner_result2 = ner(input2)\n",
    "ptags2 = compute_prediction(inputs[11], input2, ner_result2)\n",
    "ptags2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(targets[9], ptags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O\n",
      "O O\n",
      "O O\n",
      "O O\n",
      "O O\n",
      "B-LOC B-LOC\n",
      "O O\n",
      "B-PER B-PER\n",
      "I-PER I-PER\n",
      "O O\n",
      "O O\n",
      "O O\n",
      "O O\n",
      "B-ORG B-ORG\n",
      "O O\n",
      "O O\n",
      "O O\n",
      "O O\n",
      "O O\n",
      "O O\n",
      "O O\n",
      "O O\n",
      "O O\n",
      "O O\n",
      "O O\n"
     ]
    }
   ],
   "source": [
    "for targ, pred in zip(targets[9], ptags):\n",
    "  print(targ, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get detokenized inputs to pass into ner model\n",
    "# detok_inputs = []\n",
    "# for tokens in inputs:\n",
    "#   text = detokenizer.detokenize(tokens)\n",
    "#   detok_inputs.append(text)\n",
    "# # 17 min on CPU, 3 min on GPU\n",
    "# ner_results = ner(detok_inputs)\n",
    "# predictions = []\n",
    "# for tokens, text, ner_result in zip(inputs, detok_inputs, ner_results):\n",
    "#   pred = compute_prediction(tokens, text, ner_result)\n",
    "#   predictions.append(pred)\n",
    "# # https://stackoverflow.com/questions/11264684/flatten-list-of-lists\n",
    "# def flatten(list_of_lists):\n",
    "#   flattened = [val for sublist in list_of_lists for val in sublist]\n",
    "#   return flattened\n",
    "# # flatten targets and predictions\n",
    "# flat_predictions = flatten(predictions)\n",
    "# flat_targets = flatten(targets)\n",
    "# accuracy_score(flat_targets, flat_predictions)\n",
    "# 0.9920892614676191\n",
    "# f1_score(flat_targets, flat_predictions, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
